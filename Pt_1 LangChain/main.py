import re  # Importa o m√≥dulo de express√µes regulares para buscar padr√µes na string
import os  # Importa o m√≥dulo para manipular vari√°veis de ambiente do sistema
import json  # Importa o m√≥dulo JSON para converter dados entre Python e JSON
from langchain.chat_models import init_chat_model  # Importa a fun√ß√£o para inicializar o modelo de IA
from langchain_core.prompts import PromptTemplate  # Importa a classe para estruturar perguntas enviadas √† IA
from langchain_core.runnables import RunnableLambda, RunnableSequence  # Importa fun√ß√µes para criar um pipeline de execu√ß√£o


# Define a chave da API para autentica√ß√£o no servi√ßo Groq (‚ö†Ô∏è Nunca compartilhe sua API Key publicamente!)
os.environ["GROQ_API_KEY"] = "Digite sua API KEY aqui"  # üî¥ Substitua pela sua API KEY

# Inicializa o modelo de IA usando o provedor "groq"
model = init_chat_model("llama3-8b-8192", model_provider="groq")

# Cria√ß√£o de etapas do pipeline para processamento da pergunta

# 1. Valida se a pergunta cont√©m uma express√£o matem√°tica e levanta erro se n√£o for v√°lida
validation_chain = RunnableLambda(lambda x: {"question": validate_question(x["question"])})

# 2. Processa a pergunta usando a IA e gera uma resposta
ai_chain = RunnableLambda(lambda x: {**x, "answer": process_ai_question(x["question"])})

# Montagem do pipeline de execu√ß√£o
pipeline = RunnableSequence(
    validation_chain,  # Valida se √© uma pergunta matem√°tica
    ai_chain  # Se passou na valida√ß√£o, a IA processa a pergunta
)


def validate_question(question: str):
    # Express√£o regular para identificar perguntas matem√°ticas
    math_terms = r"[\d\+\-\*\/\=\(\)x]"
    is_valid = re.search(math_terms, question, re.IGNORECASE)
    if not is_valid:
        raise Exception("A pergunta deve ser sobre matem√°tica")  # Lan√ßa exce√ß√£o caso a pergunta n√£o seja v√°lida
    return question


def process_ai_question(question: str) -> str:
    # Cria um template de prompt para estruturar a pergunta antes de envi√°-la ao modelo
    prompt_template = PromptTemplate.from_template("Responda a seguinte pergunta matem√°tica: {question}")
    # Cria uma cadeia (pipeline) onde o prompt formatado √© processado pelo modelo de IA
    chain = prompt_template | model
    # Executa o pipeline passando a pergunta e recebe a resposta da IA
    answer = chain.invoke({"question": question})
    # Retorna o conte√∫do gerado pela IA
    return answer.content


def receiver(question: str):
    try:
        result = pipeline.invoke({"question": question})  # Executa o pipeline com a pergunta fornecida
        # Retorna a resposta gerada pela IA no formato JSON
        return json.dumps({
            "Pergunta": question,
            "Categoria": "matem√°tica",
            "Resposta": result["answer"]
        }, indent=4)
    except Exception as e:
        return json.dumps({
            "Erro": str(e)
        }, indent=4)


def run():
    # Solicita ao usu√°rio que insira uma pergunta matem√°tica no terminal
    question = input("Insira sua pergunta matem√°tica: ")
    # Chama a fun√ß√£o receiver para processar a pergunta e exibe a resposta ou erro
    print(receiver(question))


if __name__ == "__main__":
    run()
